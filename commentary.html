<section class="post">
  <h2>Sharpening Blunt Instruments: Exploiting Non-Linearities to Enhance Identification</h2>

  <p>
    Instrumental variables (IV) are a workhorse of causal inference in political science and international relations.
    We reach for IV when the treatment we care about is endogenous—because reverse causality, omitted variables,
    selection, or measurement issues make simple regression comparisons misleading. But IV designs are often fragile
    in practice. Instruments can be weak, they can violate exclusion, and—most commonly—the same “plausibly exogenous”
    source of variation is used across many studies to instrument many different regressors. Over time, that reuse
    exposes a basic problem: if an instrument plausibly shifts multiple determinants of the outcome, it becomes hard
    to defend the claim that it affects the outcome only through the one endogenous variable a given paper happens
    to instrument.
  </p>

  <p>
    This forthcoming paper in <em>International Studies Quarterly</em>, <strong>“Sharpening Blunt Instruments:
    Exploiting Non-Linearities to Enhance Identification,”</strong> offers a practical way to get more identification
    leverage out of common instruments—without treating them as magically single-channel. The core idea is simple:
    stop assuming the first stage is linear. In many applications, the relationship between an exogenous source of
    variation and an endogenous regressor is non-linear. If we model that first stage flexibly (using splines or
    generalized additive models), the same source of exogenous variation can generate <em>distinct</em> components of
    variation that are useful for identification.
  </p>

  <h3>A key distinction: the source of variation is not the instrument</h3>

  <p>
    A conceptual move in the paper is to separate the <strong>source of exogenous variation</strong> from the
    <strong>instrument(s)</strong> used in estimation. Researchers often treat a variable <em>z</em> as “the instrument.”
    But in practice, what powers identification is not <em>z</em> itself—it is the relationship between <em>z</em> and the
    endogenous regressor(s). If that relationship is non-linear, then different functions of <em>z</em> can capture
    different parts of the exogenous variation. Those parts can behave like distinct instruments, even though they
    originate in the same underlying source.
  </p>

  <p>
    This matters because it clarifies what many applied literatures are already doing implicitly: leveraging one
    exogenous source to learn about multiple endogenous processes—sometimes responsibly, sometimes not. The paper’s
    goal is to make that logic explicit, model it directly, and diagnose when it succeeds versus when it becomes
    numerically fragile.
  </p>

  <h3>The method: flexible first stages + control functions (2SRI)</h3>

  <p>
    Empirically, the paper develops a control-function approach based on <strong>two-stage residual inclusion (2SRI)</strong>.
    The workflow is:
  </p>

  <ol>
    <li>
      <strong>Estimate flexible first stages</strong> for each endogenous variable as a function of the exogenous source
      <em>z</em> (e.g., splines/GAMs).
    </li>
    <li>
      <strong>Extract first-stage residuals</strong> (the endogenous components not explained by the exogenous source).
    </li>
    <li>
      <strong>Include those residuals in the outcome model</strong> to correct for endogeneity while allowing the first
      stage to be non-linear.
    </li>
  </ol>

  <p>
    Modeling the first stage flexibly can improve identification in three ways:
  </p>

  <ul>
    <li>
      <strong>Stronger relevance:</strong> if the true first-stage relationship is non-linear, a flexible model captures
      it better than a linear approximation.
    </li>
    <li>
      <strong>Cleaner correction for post-instrument bias:</strong> when <em>z</em> plausibly affects the outcome through
      multiple endogenous channels, explicitly modeling those channels reduces bias from leaving them out.
    </li>
    <li>
      <strong>Potential multiple identification:</strong> under specific conditions, one exogenous source can support
      identification of more than one endogenous effect.
    </li>
  </ul>

  <p>
    That last point is where the “sharpening” metaphor comes from. A blunt instrument often fails not because it has no
    information, but because the model is too rigid to extract the information it contains.
  </p>

  <h3>When can one source identify multiple endogenous effects?</h3>

  <p>
    The paper does not claim that one instrument can always identify everything. Instead, it specifies a condition:
    identification of multiple endogenous effects is possible when the first-stage relationships generated by <em>z</em>
    are <strong>functionally distinct</strong>—not just scaled copies of each other. If the fitted components are too
    similar, the system becomes close to singular. In that case, estimates can become unstable, standard errors can
    explode, and results can be driven by numerical artifacts rather than genuine identifying variation.
  </p>

  <p>
    To make this transparent, the paper introduces diagnostics that treat multiple identification as an empirical
    question rather than an assumption. The intuition is familiar from linear algebra: if the relevant first-stage
    components are nearly collinear, you do not have enough independent variation to estimate multiple effects
    precisely.
  </p>

  <h3>A diagnostic lens: near-degeneracy and precision loss</h3>

  <p>
    A practical contribution of the paper is a screening approach for near-degeneracy, based on the conditioning of the
    system implied by the first-stage fitted terms. Rather than asserting that multiple instruments “exist,” the
    diagnostic asks whether the first-stage transformations of <em>z</em> are sufficiently independent to support the
    second stage. When the system is poorly conditioned, the paper shows how inference can become sensitive to small
    perturbations—an issue that is often invisible when researchers focus only on conventional first-stage strength
    metrics.
  </p>

  <p>
    The broader point is methodological: in multi-endogenous IV problems, <em>relevance</em> is not just about whether
    each first stage is strong on its own. It is also about whether the set of identifying components is jointly
    informative—or whether they collapse onto essentially the same variation.
  </p>

  <h3>What the paper shows in applications</h3>

  <p>
    The paper illustrates the approach with applications where researchers often rely on a single exogenous source to
    instrument multiple related regressors. The payoff is twofold. First, flexible first stages can reveal that linear
    specifications understate first-stage relationships, creating the appearance of weakness or misfit that is actually
    functional-form error. Second, explicitly modeling multiple endogenous pathways can change substantive conclusions
    by reducing post-instrument bias—especially in settings where one “instrument” is known to affect several outcomes
    and intermediate variables.
  </p>

  <p>
    Across these illustrations, the message is consistent: the strongest IV designs are not the ones with the most
    heroic exclusion claims. They are the ones that take seriously the possibility that the exogenous source moves
    several endogenous variables—and then model that structure directly.
  </p>

  <h3>Why this matters for applied research</h3>

  <p>
    This paper aims to be constructive for researchers who use IV in the real world, where clean instruments are scarce.
    If your instrument is “too powerful” in the sense that it plausibly affects many things, the standard response is
    often to abandon IV or to narrow the story until exclusion sounds believable. Our approach offers another option:
    treat that complexity as a feature of the data-generating process and build it into the identification strategy.
  </p>

  <p>
    At the same time, the paper is not a free pass. Flexible first stages and control functions do not eliminate the
    core burden of IV logic. Researchers still need to think carefully about causal pathways and ensure that the model
    accounts for relevant channels through which the exogenous source may affect the outcome. What the approach does
    provide is a clearer framework for doing that work—and diagnostics that tell you when the identification you want is
    likely to be stable versus when it is skating on near-collinearity.
  </p>

  <p>
    If you use IV regularly—or review papers that do—this is the paper’s bottom line:
    <strong>non-linear first stages are not a technical embellishment. They can be the difference between a weak or biased
    IV design and one that is both more credible and more informative.</strong>
  </p>
</section>
